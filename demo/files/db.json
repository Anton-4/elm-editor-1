[{"fileName":"quantum.md","id":"143d1170-f8ce-47b3-904d-e84191d3d717","content":"\nuuid: 143d1170-f8ce-47b3-904d-e84191d3d717\n\n## The propagator\n\nConsider a wave function $\\psi(x,t)$.\nIf we fix $t$ and let $x$ vary, the\nresult is an element $\\psi(t)$ of\n$L^2(R)$ or, more generally\n$L^2(\\text{configuration space})$.\nThus the evolution of our system in\ntime is given by a function\n$t \\mapsto \\psi(t)$.  The dynamics\nof this path in Hilbert space is\ngoverned by an ordinary differential\nequation ,\n\n$$\ni\\hbar\\frac{d\\psi}{dt} = H\\phi,\n$$\n\nNow consider bases of orthogonal\nnormalized states\n$\\{\\; \\psi_k(t_1)\\;\\}$ and\n$\\{\\; \\psi_k(t_0) \\; \\}$\nat times $t_1$ and $t_0$,\nwith $t_1 > t_0$. There is a unique linear\ntransformation $U(t_1,t_0)$\nsuch that\n$\\psi_k(t_1) = U(t_1,t_0)\\psi_k(t_0)$\nfor all $k$.\nIt must be unitary because the bases\nare orthonormal.\nThis family of transformations is\ncalled the \\term{propagator}.\nThe propagator satisfies various\nidentities, e.g., the composition law\n\n$$\nU(t_2, t_0) = U(t_2, t_1)U(t_1, t_0)\n$$\n\nas well as $U(t,t) = 1$,\n$U(t_1,t_2) = U(t_2,t_1)^{-1}$.\n\nLet us write $U(t) = U(t,0)$ for\nconvenience, and let us suppose given\nstates $\\alpha$ and $\\beta$.\nThe probability that the system finds\nitself in state $\\beta$ after time $t$\nis given by the matrix element\n\n$$\n\\bra \\beta U(t)  \\ket \\alpha\n$$\n\nThis is just the kind of information\nwe need for comparison with experiment.\n\nThe propagator, like the family of state\nvectors $\\psi(t)$, satisfies a differential\nequation -- essentially a Schroedinger equation\nfor operators. To find it, differentiate the\nequation $\\psi(t) = U(t)\\psi(0)$ to obtain\n\n$$\ni\\hbar \\frac{d\\psi}{dt} = i\\hbar \\frac{dU}{dt}\\psi(0)\n$$\n\nSubstitute (C) to obtain\n\n$$\ni\\hbar \\frac{dU}{dt}\\psi(0)  = H\\psi(t)\n$$\n\nApplying $\\psi(t) = U(t)\\psi(0)$ again, we find that\n\n$$\ni\\hbar \\frac{dU}{dt}\\psi(0) = HU\\psi(0)\n$$\n\nIf this is to hold for arbitrary $\\psi(0)$, then\n\n$$\n\\frac{dU}{dt} = -\\frac{i}{\\hbar}HU\n$$\n\nIf $H$ does not depend on time, the preceding\nODE has an immediate solution, namely\n\n\n$$\nU(t) = e^{-i(t/\\hbar) H}\n$$\n\nThink of $H$ as a big matrix, and of the expression\non the right as a big matrix exponential.\n\n\n## Notes on the free-particle propagator\n\nBelow are graphs of the real part of the\nfree-particle propagator for time\n$t = 1, 2, 4,16$.\n\n\n![xx::center](http://noteimages.s3.amazonaws.com/jim_images/propagator-t=1-63c8.png)\n\n\n![xx::centerhttp://noteimages.s3.amazonaws.com/jim_images/propagator-t=2-6feb.png)\n\n![xx::center](http://noteimages.s3.amazonaws.com/jim_images/propagator-t=4-a035.png)\n\n![xx::center](http://noteimages.s3.amazonaws.com/jim_images/propagator-t=16-e5ae.png)\n\n"},{"fileName":"test.tex","id":"7a7e54a9-70d9-4263-ba02-cb685e1fdaf8","content":"\nuuid: 7a7e54a9-70d9-4263-ba02-cb685e1fdaf8\n\n\n\\strong{Note.} This version of the MiniLaTeX\ndemo uses MathJax 3, which is much faster than\nthe 2.7.* versions and which gives a much better\nexperience when doing live editing.\n\n\\section{Introduction}\n\nMiniLatex is a subsert\nof LaTeX that can\nbe endered live in the browser using a custom parser.\nMathematical text is rendered by\n \\href{https://mathjax.org}{MathJax}:\n\n$$\n\\int_{-\\infty}^\\infty e^{-x^2} dx = \\pi\n$$\n\nThe combination of MiniLaTeX and MathJax\ngives you access to both text-mode\nand math-mode LaTeX in the browser.\n\n\nFeel free to\nexperiment with MiniLatex using this app\n\\mdash you can change the text in the\nleft-hand window, or clear it and enter\nyour own text. For more information about\nthe MiniLaTeX project, please go to\n\\href{https://minilatex.io}{minilatex.io},\nor write to jxxcarlson at gmail.\n\n\nAn improper integral:\n\n\\begin{equation}\n\\label{integral:exp}\n\\int_0^\\infty e^{-x} dx = 1\n\\end{equation}\n\n\n\\section{Images}\n\n\\image{http://psurl.s3.amazonaws.com/images/jc/beats-eca1.png}{Figure 1. Two-frequency beats}{width: 350, align: center}\n\n\\section{Lists and Tables}\n\nA list\n\n\\begin{itemize}\n\n\\item This is \\strong{just} a test.\n\n\\item \\italic{And so is this:} $a^2 + b^2 = c^2$\n\n\\item And you can do this:\n$ \\frac{1}{1 + \\frac{2}{3}} $\n\n\\end{itemize}\n\n\n"},{"id":"46332e08-5fc9-4b8f-990f-6f64ac584671","fileName":"type_theory_01.md","content":"\n# Introduction\n\n\nType theory, which has roots in logic, mathematics\nand computer science provides a single language and a\ntheoretical framework for both mathematics and logic.\nIn the context of type theory, one can formulate\nstructures such as the natural numbers, groups, rings,\nfunctions, etc., as well as propositions about these\nstructures and proofs of the propositions.\n\nIn this theory, the notion of set is replaced by the\nnotion of *type*, and the logic is a constructivist or\nintuitionistic one in which the fundamental question\nis not *Is it true?* but rather *Does it have a proof?*\nWe will explain this when we discuss the idea of\npropositions as types. This change of viewpoint\nrequires us to give up the law of the excluded middle.\nIn return, it becomes possible to produce proofs by\ncomputer program. A notable example of this is the\nproof of the four-color theorem.\n\n## The Natural numbers\n\nIn the language of sets, one can say that the natural\nnumbers is the set\n\n>> &bbN; = { {}, {{}}, {{}, {{}}}, ... }\n\nwhere on introduces the synonyms 0 = {}, 1 = {{}}, 2 =\n{{}, {{}}}, ... In this setup statements like \"0 is an\nelement of 1\" make sense, as do the statements \"0, 1,\n2, etc. are elements of **N**.\" Note that the language\nof sets has some problems. Define\n\n>> S = { x &notin; x }\n\nThen 0, 1, 2, etc are elements of S, the \"set of\nordinary sets,\" as is **N**. But we now ask \"is S\nordinary\". Let's find out. If S is an element of S,\nthen by definition it is not an element of S. So\nwe rule out this possibility. But if S is not an\nelement of S, then by definition, it is in S, also a\ncontradiction. This is *Russell's paradox.*\n\nType theory offers a another way of describing\nmathematical objects. As an exmple, we define the\n*type* &bbN; \nof natural numbers of natural numbers by\nthe following rules:\n\n>> 0 : &bbN; \nIf a : &bbN;, then S a : &bbN;\n\n\nThis is the way Peano defined the natural numbers,\nwhere one thinks of *S* as the function S : &bbN; &t\no; &bbN; that given one integer, returns the next one.\nIt is the \"successor function.\" The two rules are\ncalled *constructors,* since they tell us how to\nconstruct elements of &bbN;.\n \n\nAn expression like `a : N` is analogous to the\nstatement \"a is a member of the set &bbN;,\" but\nnonetheless different. The preferred discourse is \"a\ninhabits the type A\" or \"a is a term of A\" rather than\n\"a is an element of the type A.\" Nonetheless, we often\nlapse and say \"element.\" The difference, however, is\nmore than terminological. Inhabitants of a type have\nthat status because they were constructed by certain\nrules. Thus to speak of them apart from their type\nmakes no sense. For this reason, while we can say 0:\n&bbN;, but we cannot say 0: &bbQ;, where &bbQ; is the\ntype of rational numbers. What we can do, of course,\nis to define a function f : &bbN; &to; &bbQ; such that\nif k : &bbN; then f(k) : &bbQ; is the corresponding\nrational number.\n\nOne can proceed to endow the natural numbers as a type\nwith its familiar operations and laws. For example,\none can define addition by the rules\n\n>>     m + 0 = m\n    m + S(n) = S (m + n)\n\n\nMissing of course, are the laws, e.g, the commutative\nand associative laws. For now, we fill one obvious\ngap. Part of the definition tells us that 0 is left\nidentity for addition: m + 0 = m. To prove that it is\nalso a right identity, we proceed by induction.\nFirst, \n\n>>    0 + 0 = 0\n\nThen, assuming that \n0 + n = n, we prove that 0 + S(n) = \nS(n):\n\n>> 0 + S n = S(0 + n)  -- definition of addition\n            = S(n)         -- inductive hypothesis\n\nTo make all of this legitimate, we must add a formal\nrule for induction to those defining the type &bbN;.\n\n## Functions\n\nThe notion of function in set theory is derived from\nthe underlying notion of set. A function f : A &to;\nB is a subset &Gamma; of A x B that satisfies certain\nproperties. For each x &in; A there is a unique y &in;\nB which we call f(x). The set of pairs (x, f(x)) is the\nset &Gamma;, which we think of as the graph of f. One\nconsequence of this definition is that in set theory\nthere are noncomputeble fuctions f : &bbN; &to; &bbN;\nThese are functions whose values cannot be computed\nby any Turing machine. The proof is simple. The set of\nall possible programs for Turing machine is countable.\nTherefore the set of computable functions on the\nnatural numbers is countable. But (by the Cantor\ndiagonal argument), the set of all functions &bbN;\n&to; &bbN; is uncountable.\n\nThe notion of function in type theory is primitive.\nIt goes like this. First of all, if A and B are types,\nthen there is a type A &to; B, the type of functions\nfrom A to B. The question then arises: *is this type\ninhabited?* To give a function f of type A &to; B is \nto give a rule\nsuch that given any element of A, we can compute an \nelement of B.  Here is one rule: fix an element of \nb.  Then for all a : A, f(a) = b.  Here is another:\ngiven a : A, let f(a) = a.  Then f : A &to; A is the \nidentity function.\n\nThe rule for forming function types \ntells us that we can form\ntypes like A &to; (B &to; C). Indeed, A \nis a type and so is B\n&to; C. Therefore \nA &to; (B &to;C) is a type. We could\nalso form (A &to; B) &to; C, which is different. We\nshall agree that forming types is right-associative,\nso that when we \nwrite A &to; B &to; C, we mean\nA &to; (B &to; C). \n\nAs an example, consider the function $add_n$ defined\nusing our addition operation by the equation $add_n(m)\n= n + m$. This is a function of type &bbN; &to; &bbN;.\nConsider also the function $add$ such that $add(n)\n=add_n$. It is a function-valued function, of type\n&bbN; &to; (&bbN; &to; &bbN;) = &bbN; &to; &bbN; &to;\n&bbN;. Thus *add(n)(m) = n + m* : &bbN;\n\nThere is a convenient alternative notation for applying\nsuch functions to their arguments. If f : A &to; B\n&to; C and a : A, then f a means f applied to A. It\nis therefore a function B &to; C. If b : B, then f a\nb means (f a) applied to b, and so is an element of \nC. Sometims we shall use this notation, but we also \nfeel free to use traditional form, f(a,b).\n\n\n Here is a more interesting example;\nDefine $g(0) = 0$ and the demand that it obey the \nrule \n\n$$\ng(S(n)) = S(S(g(n)))\n$$\n\nThen we can compute $g(n)$ for any $n$.  For \nexample,\n\n>> g(S(0)) = S(S(0))\n\nMake the definitions 1 = S(0), 2 = S(S(0)), etc.\nThen the previous statement is read as g(1) = 2.\nWe recognize g as the function that doubles its\nargument.  Indeed,\n\n>> g(2) = g(S(S(0)))  \n        = S(S(g(S(0))))\n        = S(S(S(S(0))))\n        = 4\n\n\n## Induction\n\nFrom the example of the doubling function just given,\nwe can formulate a first principle of induction.\nSuppose given a type C, an element d : C, and a\nfunction e : &bbN; &to; C &to; C. Let us call this\ndata **rec**(C, d,e). We claim that the two rules\n\n>> f(0) = d \nf(S(n)) = e(n,f(n))\n\ndefine a function f : &bbN; &to; C.  This function\nis not defined merely in some \nabstract sense.  The rules provide a\nscheme for computing f(n) for any n : &bbN;. \nThe first rule defines f(0).  The second rule gives\na way of calculating f at n in terms of f at n-1,\nand so a way of reducing the computation to a \ncompuatio of f(0).  Here is an example:\n\n>> f(3) = f(S(2)))\n      = e(2,f(2))\n      = e(2,f(S(1)))\n      = e(2,e(1,f(1)))\n      = e(2,e(1,f(S(0))))\n      = e(2,e(1,e(0,f(0)))))\n      = e(2,e(1,e(0,d))))\n\nAs an example, consider the function defined \nconventionally as \n\n>> f(n) = 1 + 2 + 3 + ... + n\n\nIt is a funcition \nof type &bbN; &to; &bbN;, so C = &bbN; It satisfies \nthe recursion equation \n\n>> f(S(n)) = f(n+1)\n           =  (1 + 2 + 3 + ... + n) + (n + 1)\n           = f(n) + S(n)\n\nThus, if we set d = 0: &bbN; and \ne(n,m) = S(n) + f(n), then\nf satisfies **rec**(C,d,e).  Since\n**rec**(C,d,e) determines f, we set \n\n>> f = **rec**(C,d,e)\n\nYou might wonder whether the dependence on C \nis really needed.  It is, if we are to define \nSince **rec**(C,d,e) defines a function &bbN; &to; \nC, and since *add* : &bbN; &to; &bbN; &to; &bbN;, \nit follows that C = &bbN; &to; &bbN;  The constant \nd will be *add*(0), and since *add*(0)(m) = m, we \nmust have *add*(0) = id : &bbN; &to; &bbN; For the \nrecursion equation, note that\n\n>> *add*(S(n))(m) = *add*(n+1)(m)\n                      = n + 1 + m\n                      = S(*add*(n))(m))\n                      = (So*add*(n))(m)\nso that\n\n>> *add*(S(n))       = S o *add*(n)\n\nwhere o denotes composition of functions.  This \nmakes sense, since both S and *add*(n) have\ntype &bbN; &to; &bbN; We conclude that\n\n>> add = **rec**(&bbN; &to; &bbN;, id, e)\n\nwhere e(n,f) = Sof.\n\n### Note\n\nWe can view **rec** as a function that takes data\nfor making functions to functions:\n\n>> **rec** : DATA -> (&bbN; &to; C)\n\nWe can read the type of DATA from the arguments\nof **rec**:\n\n>> DATA = &caU;  &to; C &to;  (&bbN; &to; C &to; C)\n\nHere &caU; is something we have not seen before: a \n*universe*, which is a kind of type of types.  Thus \nwe can say things like C : &caU; or \n\n>> If A : &caU; and B : &caU;, then (A &to; B) : &caC;\n\nNote the analogy with the rule\n\n>> If n : &bbN; then S n : &bbN;\n\nIn the latter case, S is a constructor for natural \nnumbers.  In the former, &to; is a  constructor for \ntypes.  One has to be careful about the notion of \nuniverse.  If fact there is an infinite hierarchy \nof universes\n\n>> &caU; &sub; &caU;' &sub; &caU;'' &sub; ...\n\nThis hierarchy is \nnecessary to rule out paradoxes akin to \nRussell's paradocs in the theory of sets.  In any case, \nreturning to our analysis of **rec**, we now know\nits full type:\n\n>> **rec** : &caU; &to; C &to; (&bbN; &to; C &to; C) &to; (&bbN; &to;  C)\n\nDespite the fearsome apparance of its type, keep\nin mind that we were led naturally to it by \nexaminging the arguments. \n\n## Lambda calculus\n\nIt is time to take a break and\nexamine more closely what we mean by a function f : A\n&to; B. Our working notion is that it is some kind of\nrule that given an element of A produces an element\nof B. We can formalize this using the lambda calculus,\na theory developed by the logician Alonzo Church to\ndefine the notion of computable function. Computable\nin the sense of Church turns out to be computable in\nthe sense of Turing, though the underlying models are\ncompletely different. Fundamental in Church's theory\nis the notion of a *lambda term*, which is defined as\nfollows\n\n1. There is an infinite list of variables x, y, z, \n... .  Variables are terms.\n\n2. If x is a variables and s is a lambda term,\nthen the *abstraction* $\\lambda x. s$ is a term.\n\n3. If $s$ and $t$ are lambda terms, then so is the\nlambda term $s\\ t$, pronounced \"$s$ applied to $t$\"\n\nThese rules define the formal grammar of the lambda \ncalculus. By repeated application of the rules, one \ncan build up terms such as\n\n>> &lambda; x.x \n(&lambda; x.x) y\n\nThe first term above is an abstraction, and the second is\napplication of the abstraction to a term. This is \n the pure lambda calculus.  But we can also \ntake a more liberal view, admitting\nas terms expressions from some other domain such as\nbasic algebra. In this system, we can form terms\n\n>> &lambda; x.(x + 5)\n(&lambda; x.(x+5)) 2\n\n**Note.** The expressions $(\\lambda x.x)$ and $(\\lambda\ny.y)$ are regarded as equivalent, or more precisely,\n*alpha-equivalent*. Alpha-equivalence is expresses the\nprinciple that the names of abstracted variables is\nnot significant.\n\nThe notion of application is given meaning through the\nmechanism of &beta;-reduction. Let $s[x := t]$ denote\nthe result of substituting the term t for the variable\nx everywhere in the expression s. Then\n\n$$\n(\\lambda x.s)\\ t \\to_\\beta s[x=t]\n$$\n\nThus we have the reductions\n\n$$\n(\\lambda x.x) y \\to_\\beta y\n$$\n\nand \n\n$$\n(\\lambda x.(x+5)) 2 \\to_\\beta 2 + 5\n$$\n\nThe first reduction tells us that the function in\nquestion is the identity. As for the second, if we are\nin a system that in addition to the standard rules of\nbeta-reduction includes certain rules of arithmetic\nthen the above could be reduced further to the integer 7. \nIn that case we could write\n\n$$\n(\\lambda x.(x+5)) 2 \\to_{\\beta} 2 + 5 \\to_{\\beta} \n\n7\n$$\n\nIn each of the last two examples, the sequence of \nbeta-reductions terminates, reaching a term for \nwhich no further reductions are possible. Such a \nterm is said to be in *normal form*.\n\nThe next example shows that there are terms\nwith non-terminating sequences of beta reductions. Let\n$\\omega = \\lambda x.x\\ x$, by which we mean $\\lambda\nx.(x\\ x)$. Thus $\\omega$ is the function that applies\nits argument to itself. We have inserted parentheses\nto make the syntax clear to human readers. However,\nthis is unnecessary: the scope of a lambda abstraction\nextends as far to the right as possible. Now observe\nthat\n\n$$\n(\\lambda x.x\\ x)\\ s \\to_\\beta s\\ s\n$$\n\nThus $\\omega\\ s \\to_\\beta s\\ s$, and so \n\n$$\n\\omega\\ \\omega \\to_\\beta \\omega\\ \\omega\n$$\n\nTherefore, if we set $\\Omega = \\omega\\ \\omega$,\nwe have the infinite reduction sequence\n\n$$\n\\Omega \\to_\\beta \\Omega \\to_\\beta \\Omega \\to_\\beta \n\\cdots\n$$\n\nLater we will introduce the *typed \nlambda calculus*. \nIts rules forbid the formation of $\\omega$; \nmore generally, its rules guarantee that \nbeta reduction terminates.\nConsequently, in the typed lambda calculus, all terms have a normal form.\nThis normal form is unique modulo alpha-equivalence.\n\n### Free and bound variabls\n\nConsider the expression \n\n$$\n\\lambda x.(x + y)\n$$\n\nIn it, the variable $x$ is *bound*, whereas the \nvariable $y$ is *free*.  Consider the subsitution\n\n$$\n\\lambda x.(x + y)[y = x*x]\n$$\n\n\nCare must be taken with bound variables\nwhen applying beta-reduction. Consider the expression \n\n$$\n\\lambda x.(x + y)[y := x*x]\n$$\n\nIt is alpha-equivalent to \n\n$$\n\\lambda u.(u + y)[y = x*x]c\n$$\n\nand we have\n\n$$\n\\lambda u.(u + y)[y = x*x] \\to_\\beta \n   \\lambda u.(u + x*x)\n$$\n\nHowever, if we substitute before applying our alpha\nequivalence, we find\n\n$$\n\\lambda x.(x + x*x)\n$$\n\nThe correct path is to rename variables, then apply\nbeta reduction. Renaming variables in the right way\nensures that free variables do not become bound as a\nresult of substitution. One way to ensure this in a \nsubstitution $t[x := s]$ is to rename variables so \nthat all bound variables in  $t$ are distinct from \nall free variables in $s$.\n\n\n\n## Propositions as types\n\nIn the introduction, we claimed that type theory,\nin addition to being a sufficiant language for\nmathematical objects, is also a sufficient language\nfor logic.  In this chapter we will see why this is\nso. The idea is a kind of dictionary.  It begins \nlike this\n\n````\n        LOGIC                    TYPE THEORY\n        -----------------------------------------\n        Proposition              Type\n        Logical connective       Type constructor\n        Proof, evidence          Inhabitant\n````\n\nConsider, therfore, propositions P and Q.  In \nclassical logic, there are only two things that\nwe can say about a proposition: it is true, or \nits false.  In constructivist or intuitionsitic\nlogic, the key notion is not truth, but rather\n\"having a proof\" or \"not having a proof.\"\nWe we view inhabitants of P as \"witnesses\" \nor proofs.\n\nLet us now see how we can represent *modus ponens*,\none of the founding principles of classical logic, in\ntype theory. Modus ponens says that if P is true and\nP implies Q, then Q is true. We model the implication\nP &to; by the function type P &to; Q.\n\nTo say that an implication is valid is to say that\nthe statement that the type P &to; Q is inhabited.\nLet us suppose that f : P &to; Q and that a : P\n(because P holds and therefore the corresponding \ntype is inhabited.  But then f(a): Q \nso that Q holds.  This is the sought-for \nconclusion.           \n\nAs a further  example, \nconsider the proposition $P \\to \nP$.  To prove it in the contenxt of type theory,\nwe must show that the type $P \\to P$ is inhabited. \nBut this is easy:\n\n$$\n  id = \\lambda x.x : P \\to P\n$$\n\nThere is, however, more to the story. While it is\ntrue that we have proved $P \\to P$ for the particular\nproposition $P$, we would like to prove it for *all*\npropositions. To do so, we need an more powerful\nidentity function, one that works for all propositions,\n$P$, so something like this:\n\n$$\n  id(P) = \\lambda x.x : P \\to P\n$$\n\nSince the argument of $id$ is a type, the domain of \nthe function $id$ is a universe ${\\mathcal U}$. \nWhat about the range?  The value of $id$ at $P$ is \nan element of $P \\to P$, while the value of $id$\nat $Q$ is an element of $Q \\to Q$.  This seems at \nfirst a bit odd: the range of the function depends\non the value of the argument.  \nTo make sense of this, we introduce the notion\nof a $\\Pi$-type.  Suppose given\na family of types  $B: A \\to {\\mathcal U}$.  \nThen we form the $\\Pi$-type \n\n$$\n\\Pi_{x: A}B(x)\n$$  \n\nAn element $f$ of this type has the property that \nif $x:A$, then $f(x):B(x)$. As an example, consider\nthe type\n\n$$\n{\\mathcal P} = \\Pi_{P:{\\mathcal U}} (P \\to P)\n$$\n  \nIt has an element \n\n$$\n\\lambda P.(\\lambda (x:P). x)\n$$   \n\nSince the type ${\\mathcal P}$ is inhabited, the\nproposition ${\\mathcal P}$ is proved. This is the \nproposition that for all propositions $P$, $P \\to \nP$.\n\nIn the course of formulating a type-theoretic proof \nof proposition ${\\mathcal P}$, we have discovered \nan important principle: the univeral quantfier \n$\\forall$ in logic can be encoded\nby $\\Pi$ types.  \n\n\n\n\n        \n### Remark\n\nThe counterpart of $\\Pi$ type in mathematics\nis the notion of a fiber space $p : B \\to A$, \nwritten $B/A$ and pronounced \"$B$ over $A$.\"\nA section of $B$ over $A$ is a function \n$ f : A \\to B$ such that $f(x) \\in B_x$,\nwhere the fiber $B_x$ is the set $p^{-1}(x)$.\n\n## Propositional Calculus\n\nBefore taking up existential quantification,\nlet us add the remaining operations of the \npropositional calculus to our dictionary:\n\n\n>>        LOGIC         TYPE THEORY\n        ----------------------------------------\n        P &to; Q              P &to; Q\n        P &or; Q                P + Q \n        P &and; Q                P x Q \n        False                    &boolzero;\n        True                      &boolone;\n        &not; P                    P &to; &boolzero;\n\nWe must explain the meaning of the types &boolzero;\nand &boolone;. The tpe &boolzero;, pronounced \"empty\"\nis a type with no constructors. Thus it has no\nelements. The type &boolone;, pronounced \"unit,\"\nhas the single constructor &boolone; : &boolone; and\ntherefore has just one element, the element \n&boolone;.\n\n \n### Sum types\n\nThe sum type $A + B$ is a type that incorporates \nthe elements of $A$ and the elements of $B$.  It \nhas two constructors, $left : A \\to A + B$ and \n$right : B \\to A + B$. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}]